log_dir: logs
log_level: INFO
define: &max_unsynced_time 0.01
define: &video_file null
#define: &video_file ./videos/test.mp4
define: &video_width 1280
define: &video_height 720

video:
  max_unsynced_time: *max_unsynced_time
  video_file: *video_file
  width: *video_width
  height: *video_height
  input_device: 0
  use_video: true
  processing_size: 1
  max_fps: 20
  output_virtual_cam: false
  output_window: true
  store: false
  store_format: "avi"
  converter:
    cls: stream_processing.models.Avatar
    width: *video_width
    height: *video_height
    ws_host: 0.0.0.0
    ws_port: 8888
    app_port: 3000
    start_chrome_renderer: true
    use_chrome_extension: true
    show_chrome_window: false
    avatar_uri: ./default_avatar.glb
    # store mediapipe's detection results for facial expression / head movement to an array in a JSON file in the log-dir:
    # NOTE that the last entry in the array will be an empty object, i.e. without "blendshapes" field!
#    detection_log: detectionLog.json

audio:
  max_unsynced_time: *max_unsynced_time
  video_file: *video_file
  use_audio: true
  output_buffersize: 1200
  processing_size: 9600
  record_buffersize: 1200
  sampling_rate: 16000
  input_device: MacBook Pro-Mikrofon
  output_device: MacBook Pro-Lautsprecher
  store: false
  converter:
    cls: stream_processing.models.KnnVC
    target_feats_path: ./target_feats/john.pt
    device: cpu
    n_neighbors: 4
    prev_audio_queue:
      max_samples: 9600
    interpolator:
      n_samples: 450
      weight: 0.75
    vad:
      frame_length: 2048
      hop_length: 512
      threshold: 0.025
